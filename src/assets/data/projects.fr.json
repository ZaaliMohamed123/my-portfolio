[
  {
    "id": 8,
    "title": "AL-MUQARRIR",
    "subtitle": "Transcription Automatique et Génération de Procès-Verbaux",
    "description": "Plateforme web alimentée par l'IA pour la transcription de réunions en temps réel, la diarisation des locuteurs et la génération automatisée de procès-verbaux avec analyse sémantique. Support de l'arabe, du français, de l'anglais et du darija avec capacités multilingues et RTL.",
    "longDescription": "AL-MUQARRIR est une solution complète alimentée par l'IA qui transforme les enregistrements de réunions en documentation structurée et exploitable. La plateforme propose l'identification des locuteurs en temps réel à l'aide de PyAnnote.audio, une transcription de haute précision avec le modèle Whisper d'OpenAI, et une analyse sémantique intelligente alimentée par l'API Gemini de Google. Le système génère automatiquement des procès-verbaux professionnels avec extraction de thèmes, analyse de sentiment et identification des problèmes/opportunités. Avec le support de plusieurs langues, y compris l'arabe avec formatage RTL, la plateforme offre une exportation transparente vers les formats PDF et DOCX. Construite avec une interface Angular moderne et un backend Flask robuste, AL-MUQARRIR simplifie l'ensemble du processus de documentation des réunions, de l'enregistrement à la distribution.",
    "logo": [
      "/assets/media/projects/al-muqarrir/small_Logo.svg",
      "/assets/media/projects/al-muqarrir/big_Logo.svg"
    ],
    "thumbnail": "",
    "images": ["/assets/media/projects/al-muqarrir/home.png"],
    "videoTuto": "/assets/media/projects/al-muqarrir/Video_Tuto_Al-Muqarrir.mp4",
    "demoGif": "",
    "technologies": [
      "Whisper AI",
      "PyAnnote",
      "Gemini API",
      "Flask",
      "Angular",
      "SQLite",
      "Bootstrap",
      "JWT",
      "Python"
    ],
    "categories": ["ai", "web", "data_science"],
    "github": "https://github.com/SEKALDouaa/AL-MUQARRIR",
    "goToPage": "/projects/8"
  },
  {
    "id": 7,
    "title": "Cura",
    "subtitle": "Système de Gestion Médicale Alimenté par l'IA",
    "description": "Application web complète basée sur Flask pour gérer les médicaments des patients, les pathologies et les rendez-vous avec détection des interactions médicamenteuses (DDI) alimentée par l'IA utilisant l'apprentissage automatique et Google Gemini AI. Comprend le suivi des médicaments, les relations mentor-patient, la gestion de pharmacie et les vérifications intelligentes de sécurité des médicaments.",
    "longDescription": "Cura est une plateforme sophistiquée de gestion des soins de santé qui combine le suivi traditionnel des médicaments avec une technologie d'IA de pointe. Le système utilise une approche à deux niveaux pour l'analyse des interactions médicamenteuses : un réseau neuronal MLP PyTorch local avec des empreintes moléculaires de RDKit pour la détection d'interactions basée sur les probabilités, et Google Gemini AI pour générer des alertes de sécurité adaptées aux patients et des conseils médicaux contextuels. La plateforme permet aux patients de gérer leurs médicaments avec suivi de dosage, de configurer des rappels automatisés, de surveiller les conditions médicales (pathologies) et de planifier des rendez-vous avec les professionnels de santé. Le système de mentorat connecte les patients avec des mentors pour un soutien continu, tandis que des fonctionnalités avancées d'autocomplétion avec correspondance floue aident les utilisateurs à trouver rapidement des médicaments. Construit avec Flask et SQLAlchemy, Cura fournit un contrôle d'accès basé sur les rôles, un hachage sécurisé des mots de passe avec Bcrypt et une journalisation complète de la prise de médicaments. Le système s'intègre avec des API externes, notamment PubChem pour les données moléculaires, et fournit une analyse DDI en temps réel pour améliorer la sécurité des patients.",
    "logo": ["assets/media/projects/cura/cura_logo_no_back.svg"],
    "thumbnail": "",
    "images": ["/assets/media/projects/cura/dashboard_cura.png"],
    "videoTuto": "/assets/media/projects/cura/Video_Tuto_Cura.mp4",
    "demoGif": "",
    "technologies": [
      "PyTorch",
      "Gemini API",
      "Flask",
      "SQLAlchemy",
      "Bootstrap",
      "SQLite",
      "NumPy",
      "Pandas",
      "Python"
    ],
    "categories": ["health_tech", "ai", "web", "data_science", "dl"],
    "github": "https://github.com/ZaaliMohamed123/Cura",
    "goToPage": "/projects/7"
  },
  {
    "id": 6,
    "title": "Système de Détection d'Intrusion Réseau en Temps Réel",
    "subtitle": "Surveillance de Sécurité Réseau Alimentée par le ML",
    "description": "Système de détection d'intrusion réseau en temps réel qui capture et analyse les paquets réseau à l'aide de l'apprentissage automatique. Construit avec Apache Kafka pour le streaming de messages, PyShark pour la capture de paquets et Scikit-learn pour la classification du trafic. Le système fournit une surveillance réseau continue avec détection automatisée des menaces et journalisation complète.",
    "longDescription": "Ce système de sécurité réseau avancé implémente la détection d'intrusion en temps réel en capturant les paquets réseau en direct, en les analysant à l'aide de modèles d'apprentissage automatique entraînés et en enregistrant les résultats pour l'analyse de sécurité. L'architecture exploite Apache Kafka pour une livraison et un traitement fiables des messages, assurant une gestion évolutive du trafic réseau à volume élevé. PyShark capture les paquets des interfaces réseau, extrayant les caractéristiques pertinentes telles que les types de protocole, les tailles de paquets et les informations de timing. Les données prétraitées sont introduites dans un classificateur basé sur Scikit-learn qui prédit les types de trafic et identifie les menaces de sécurité potentielles. Le système dispose d'une journalisation automatisée des données avec des prédictions enregistrées dans des fichiers Excel pour l'analyse historique et les pistes d'audit. Construit avec Python, la plateforme fournit un accès basé sur les rôles aux journaux archivés et prend en charge la surveillance continue de l'activité réseau. Le modèle d'apprentissage automatique est entraîné pour détecter divers types d'intrusions réseau, notamment les attaques DoS, les scans de ports et les tentatives d'accès non autorisées, ce qui le rend adapté à la surveillance de la sécurité des réseaux d'entreprise.",
    "logo": [""],
    "thumbnail": "",
    "images": ["/assets/media/projects/network-intrusion-detection/Data_flow_diagram.png"],
    "videoTuto": "",
    "demoGif": "",
    "technologies": [
      "Python",
      "Apache Kafka",
      "PyShark",
      "Pandas",
      "Scikit-learn",
      "Joblib",
      "NumPy"
    ],
    "categories": ["ml", "data_science", "big_data"],
    "github": "https://github.com/ZaaliMohamed123/Real-time-Network-Intrusion-Detection-System-Using-Machine-Learning",
    "goToPage": "/projects/6"
  },
  {
    "id": 5,
    "title": "LipReader",
    "subtitle": "Application de Lecture Labiale Alimentée par l'IA",
    "description": "Application d'apprentissage profond qui convertit les mouvements des lèvres en texte en utilisant la vision par ordinateur avancée et les réseaux de neurones. Construite avec TensorFlow pour l'implémentation du modèle et Streamlit pour une interface utilisateur interactive. Le système utilise des réseaux de neurones convolutifs 3D combinés avec des couches LSTM pour prédire avec précision le texte à partir des mouvements des lèvres basés sur la vidéo.",
    "longDescription": "LipReader est une application innovante d'apprentissage profond qui effectue la lecture labiale en temps réel à partir de vidéos, traduisant la parole silencieuse en texte lisible. Le système exploite une architecture sophistiquée combinant des réseaux de neurones convolutifs 3D (3D-CNN) pour l'extraction de caractéristiques spatiales des images vidéo avec des couches de mémoire à long terme (LSTM) pour la modélisation de séquences temporelles. Cette approche hybride permet au modèle de capturer à la fois les caractéristiques visuelles des formes des lèvres et la dynamique temporelle des modèles de parole. Construit sur TensorFlow, le modèle traite les vidéos image par image, extrayant les caractéristiques faciales et les mouvements des lèvres pour générer des prédictions de texte précises. L'application dispose d'une interface utilisateur intuitive basée sur Streamlit qui permet aux utilisateurs de télécharger des vidéos ou de sélectionner parmi des échantillons pré-chargés pour voir les résultats de lecture labiale en temps réel. Le système comprend des utilitaires complets de prétraitement vidéo, des fonctions de mappage de caractères et le traitement des données d'alignement pour garantir une grande précision. Développé comme preuve de concept pour les technologies d'assistance et les applications d'accessibilité, LipReader démontre le potentiel de l'apprentissage profond pour aider les personnes malentendantes et permettre des systèmes de communication silencieux. Le projet présente une expertise en vision par ordinateur, modélisation séquence-à-séquence et développement d'applications interactives.",
    "logo": ["/assets/media/projects/LipReader/logo.jpeg"],
    "thumbnail": "",
    "images": ["/assets/media/projects/LipReader/interface.png"],
    "videoTuto": "/assets/media/projects/LipReader/video_tuto_lipReader.mp4",
    "demoGif": "",
    "technologies": ["TensorFlow", "Streamlit", "Python", "LSTM", "CNN", "NumPy"],
    "categories": ["dl", "ai", "data_science"],
    "github": "https://github.com/ZaaliMohamed123/Lip-Reader",
    "goToPage": "/projects/5"
  },
  {
    "id": 4,
    "title": "NER - Reconnaissance d'Entités Nommées",
    "subtitle": "Système de Détection d'Entités Multi-Modèles",
    "description": "Application avancée de reconnaissance d'entités nommées qui identifie et classifie les entités dans le texte à l'aide de plusieurs modèles d'apprentissage profond et de traitement du langage naturel. Construite avec Streamlit pour une interface interactive, le système emploie des réseaux neuronaux basés sur Keras, spaCy et Stanza pour extraire des entités telles que les personnes, les organisations, les lieux et les catégories diverses à partir de texte non structuré.",
    "longDescription": "Ce système complet de reconnaissance d'entités nommées met en œuvre plusieurs modèles de traitement du langage naturel de pointe pour identifier et classifier les entités nommées dans le texte avec une grande précision. L'application dispose d'une interface Streamlit conviviale qui permet aux utilisateurs de saisir du texte et de recevoir des prédictions d'entités en temps réel à partir de quatre modèles différents, chacun avec des forces uniques. Le premier modèle basé sur Keras se spécialise dans la détection des entités géographiques, des organisations et des personnes en utilisant une architecture de réseau neuronal profond avec des couches d'embedding personnalisées. Le deuxième modèle Keras se concentre sur les entités diverses, les personnes et les lieux avec des capacités de classification améliorées. Le modèle spaCy exploite des modèles de langage pré-entraînés pour une reconnaissance d'entités rapide et précise à travers plusieurs types d'entités. Le modèle Stanza, développé par Stanford NLP, fournit des capacités robustes de reconnaissance d'entités multilingues. Le système traite le texte d'entrée à travers chaque modèle simultanément, présentant des résultats comparatifs qui mettent en évidence les frontières et classifications d'entités. L'application comprend des algorithmes sophistiqués de prétraitement de texte, de tokenisation et de détection des frontières d'entités. Construite avec Python, la plateforme démontre une expertise en traitement du langage naturel, développement de modèles d'apprentissage profond et analyse comparative de modèles. L'approche multi-modèles permet aux utilisateurs de valider la reconnaissance d'entités à travers différents frameworks, assurant une plus grande confiance dans les tâches d'extraction d'entités. Ce projet est particulièrement précieux pour les applications d'analyse de documents, d'extraction d'informations, de catégorisation de contenu et de systèmes d'annotation de texte automatisés.",
    "logo": [""],
    "thumbnail": "",
    "images": ["/assets/media/projects/ner/interface.png"],
    "videoTuto": "/assets/media/projects/ner/video_tuto_ner.mp4",
    "demoGif": "",
    "technologies": ["Python", "Streamlit", "Keras", "spaCy", "Stanza", "TensorFlow", "NumPy"],
    "categories": ["dl", "ai", "data_science"],
    "github": "https://github.com/ZaaliMohamed123/Named-Entity-Recognition-NER-",
    "goToPage": "/projects/4"
  },
  {
    "id": 3,
    "title": "Prédiction du Niveau d'Anxiété",
    "subtitle": "Plateforme d'Évaluation de la Santé Mentale Basée sur le ML",
    "description": "Plateforme complète pour analyser et prédire les niveaux d'anxiété basée sur les traits psychologiques, les facteurs démographiques et les informations comportementales. Construite avec Python et Scikit-learn, le système utilise plusieurs modèles d'apprentissage automatique pour classifier les niveaux d'anxiété en utilisant les réponses au questionnaire de trouble d'anxiété généralisée (GAD). Comprend un backend Flask et une interface Bootstrap pour une expérience utilisateur interactive.",
    "longDescription": "Cette plateforme avancée d'évaluation de la santé mentale analyse et prédit les niveaux d'anxiété chez les individus en utilisant des techniques d'apprentissage automatique appliquées aux données psychologiques, démographiques et comportementales. Le système traite les réponses à un questionnaire de trouble d'anxiété généralisée (GAD) et classifie l'anxiété en plusieurs niveaux de gravité. Le projet met en œuvre un pipeline complet de science des données comprenant le prétraitement des données avec gestion des valeurs manquantes, l'encodage des caractéristiques catégorielles à l'aide de LabelEncoder, et l'ingénierie de caractéristiques pour extraire des modèles significatifs. La plateforme évalue plusieurs algorithmes d'apprentissage automatique, notamment Random Forest, Support Vector Machines (SVM), Gradient Boosting et d'autres, par validation croisée pour déterminer les performances optimales. Le réglage des hyperparamètres est effectué à l'aide de GridSearchCV pour optimiser la précision du modèle, le modèle SVM final atteignant des performances de classification élevées. Le système comprend une visualisation extensive des données à l'aide de Matplotlib et Seaborn pour illustrer les distributions d'âge et les relations catégorielles avec les niveaux d'anxiété. Le backend Flask fournit des endpoints RESTful pour les prédictions, tandis que l'interface utilisateur basée sur Bootstrap offre une expérience de questionnaire intuitive avec des prédictions de niveau d'anxiété en temps réel et des recommandations personnalisées en matière de santé mentale. La classe ModelPipeline encapsule toute la logique de prétraitement et de prédiction, permettant un déploiement transparent et une évolutivité. La plateforme démontre une expertise en analyse de données de santé mentale, apprentissage supervisé, optimisation de modèles et développement full-stack. Ce projet répond au besoin critique d'outils accessibles de dépistage de la santé mentale et présente l'application de la science des données dans les domaines de la santé et du bien-être.",
    "logo": [""],
    "thumbnail": "",
    "images": ["/assets/media/projects/anxiety-prediction/interface.png"],
    "videoTuto": "/assets/media/projects/anxiety-prediction/video_tuto_anxiety.mp4",
    "demoGif": "",
    "technologies": [
      "Python",
      "Scikit-learn",
      "Flask",
      "Bootstrap",
      "Joblib",
      "Pandas",
      "NumPy",
      "Matplotlib",
      "Seaborn"
    ],
    "categories": ["ml", "data_science", "health_tech", "web"],
    "github": "https://github.com/ZaaliMohamed123/Analysis-and-Prediction-of-Anxiety-Levels-Using-Machine-Learning-Techniques",
    "goToPage": "/projects/3"
  },
  {
    "id": 2,
    "title": "Détection de Fausses Nouvelles",
    "subtitle": "Classificateur d'Authenticité des Nouvelles Alimenté par le ML",
    "description": "Système d'apprentissage automatique qui classifie les articles de presse comme faux ou réels en utilisant le traitement du langage naturel et la régression logistique. Construit avec Python et Scikit-learn, le système utilise la vectorisation TF-IDF et des techniques de prétraitement de texte, notamment la lemmatisation et la suppression des mots vides, pour analyser le contenu des actualités à partir d'un ensemble de données de compétition Kaggle contenant des milliers d'articles étiquetés.",
    "longDescription": "Ce système complet de détection de fausses nouvelles exploite l'apprentissage automatique et le traitement du langage naturel pour combattre la désinformation en classifiant les articles de presse comme fiables ou non fiables. Le projet traite un ensemble de données Kaggle à grande échelle contenant des articles de presse avec des informations sur l'auteur, les titres et le contenu complet. Le pipeline de prétraitement des données fusionne les champs auteur, titre et texte en une colonne de contenu unifiée pour une ingénierie de caractéristiques complète. Les techniques avancées de prétraitement de texte incluent la lemmatisation Porter pour réduire les mots à leurs formes racines, la suppression des mots vides pour éliminer les mots courants mais peu informatifs, et le filtrage des caractères non alphabétiques pour nettoyer les données textuelles. Le système utilise la vectorisation TF-IDF (Term Frequency-Inverse Document Frequency) pour convertir le texte prétraité en caractéristiques numériques qui capturent l'importance des mots dans le corpus. Un modèle de régression logistique est entraîné sur les caractéristiques transformées, choisi pour son interprétabilité et son efficacité dans les tâches de classification binaire. Le modèle atteint une grande précision sur les ensembles de données d'entraînement et de test, démontrant des capacités de généralisation robustes. Le projet comprend une fonction de système prédictif qui accepte de nouveaux articles de presse et renvoie des prédictions d'authenticité en temps réel. L'implémentation présente une expertise en exploration de texte, extraction de caractéristiques, apprentissage supervisé et construction d'applications pratiques de traitement du langage naturel. Ce projet répond au défi sociétal critique de lutte contre la désinformation et démontre l'application de la science des données dans la promotion de la littératie médiatique et de l'intégrité de l'information. Le système peut être intégré dans des plateformes d'actualités, des outils de surveillance des médias sociaux ou des applications de vérification des faits pour aider les utilisateurs à identifier les informations potentiellement fausses.",
    "logo": [""],
    "thumbnail": "",
    "images": [""],
    "videoTuto": "",
    "demoGif": "",
    "technologies": ["Python", "Scikit-learn", "Pandas", "NumPy", "NLTK", "TF-IDF"],
    "categories": ["ml", "data_science", "ai"],
    "github": "https://github.com/ZaaliMohamed123/Fake-News-Prediction",
    "goToPage": "/projects/2"
  },
  {
    "id": 1,
    "title": "GESTAG",
    "subtitle": "Système de Gestion des Stagiaires et des Stages",
    "description": "Application web complète pour gérer les stagiaires, les stages et les inscriptions développée avec Java EE et base de données Oracle. Le système rationalise l'ensemble du cycle de vie des stages, de la sélection des étapes à l'inscription des stagiaires, avec tri dynamique, gestion de liste d'attente et suivi automatisé du statut. Construit avec une interface moderne utilisant HTML, CSS et JavaScript, et conçu dans Figma.",
    "longDescription": "GESTAG est une application web robuste de niveau entreprise conçue pour gérer le cycle de vie complet des stagiaires, des stages et des processus d'inscription pour les établissements d'enseignement et les organisations. Construite sur une architecture Java/JEE avec une base de données relationnelle Oracle en backend, le système implémente une logique métier sophistiquée via des procédures stockées et des fonctions en PL/SQL. L'application dispose d'une interface multi-fenêtres commençant par une authentification sécurisée, suivie d'un écran de sélection de stages intelligent qui affiche les stages disponibles avec des capacités de tri dynamique par type ou date de début. Les utilisateurs peuvent consulter des informations détaillées sur les stages sélectionnés, y compris la liste complète des stagiaires inscrits et les places disponibles. Le workflow d'inscription comprend la création automatique de stagiaires s'ils n'existent pas dans le système, avec validation complète des données et vérification des doublons. Le système implémente une gestion intelligente de la capacité, attribuant automatiquement les stagiaires à une liste d'attente (code de position 3) lorsque les stages atteignent leur capacité maximale, ou confirmant l'inscription (code de position 2) lorsque des places sont disponibles. Après validation, le système met automatiquement à jour les compteurs d'inscriptions et maintient l'intégrité des données dans les tables liées. L'application comprend six fenêtres soigneusement conçues : authentification de connexion, sélection de stages avec tri, informations détaillées sur les stages avec listes de stagiaires, interface d'inscription, formulaire de création de nouveaux stagiaires et messagerie de confirmation. L'interface est conçue dans Figma pour une expérience utilisateur optimale et implémentée à l'aide de HTML, CSS et JavaScript pour des interactions réactives. Le backend exploite Java EE pour un traitement côté serveur robuste et les puissantes capacités de base de données relationnelle d'Oracle. Ce projet démontre une expertise en développement d'applications d'entreprise full-stack, conception et optimisation de bases de données, implémentation de logique métier et création d'interfaces conviviales pour des workflows administratifs complexes. Développé en collaboration avec une équipe, GESTAG présente des pratiques professionnelles d'ingénierie logicielle, notamment le contrôle de version et le développement collaboratif.",
    "logo": [""],
    "thumbnail": "",
    "images": ["/assets/media/projects/gestag/interface.png"],
    "videoTuto": "/assets/media/projects/gestag/video_tuto_gestag.mp4",
    "demoGif": "",
    "technologies": ["Figma", "HTML", "CSS", "JavaScript", "Java EE", "PL/SQL", "Oracle"],
    "categories": ["web"],
    "github": "https://github.com/ZaaliMohamed123/GESTAG",
    "goToPage": "/projects/1"
  }
]
